{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import camel_tools\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from camel_tools.ner import NERecognizer\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar, dediac_bw\n",
    "\n",
    "    \n",
    "path = \"C://Users//youss//Desktop//My Crap//Geek Mode//Projects//Coding Projects//My Personal Projects//AI Projects//AIC Arabic TSUM//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(path + 'Dataset//AIC Val//ds.jsonl', lines=True)\n",
    "\n",
    "with open(Path(path + \"Preprocessing//Stopwords//Stopwords_List_HF.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    arabic_stopwords = set(f.read().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Get_NER(text):\n",
    "#     ner = NERecognizer.pretrained()\n",
    "#     return ner.predict(text)\n",
    "\n",
    "def Tokenize(text):\n",
    "    return simple_word_tokenize(text)\n",
    "\n",
    "def Lemmatize(tokens):\n",
    "    lemmatizer = camel_tools.utils.lemmatize.Lemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def DeDiacritics(text):\n",
    "    return dediac_ar(text)\n",
    "\n",
    "def Remove_SW(tokens, stopwords):\n",
    "    removed_tokens = []\n",
    "    passed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in stopwords:\n",
    "            removed_tokens.append(token)\n",
    "        else:\n",
    "            passed_tokens.append(token)\n",
    " \n",
    "    return passed_tokens, removed_tokens\n",
    "\n",
    "def Remove_SW_Camel(tokens, stopwords):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = DeDiacritics(text)\n",
    "    tokens = Tokenize(tokens)\n",
    "    tokens, rm_tokens = Remove_SW(tokens, arabic_stopwords)\n",
    "    # tokens = Lemmatize(tokens)\n",
    "    return tokens,rm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Passed(word, output):\n",
    "    with open(output, 'a', encoding='utf-8') as file:\n",
    "        file.write(word + '\\n')\n",
    "        \n",
    "def Save_Removed(word, output):\n",
    "    with open(output, 'a', encoding='utf-8') as file:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "pass_pg_path = path + 'Debug/Passed_Tokens/passed_paragraphs.txt'\n",
    "pass_sum_path = path + 'Debug/Passed_Tokens/passed_summaries.txt'\n",
    "\n",
    "removed_pg_path = path + 'Debug/Removed_Tokens/removed_paragraphs.txt'\n",
    "removed_sum_path = path + 'Debug/Removed_Tokens/removed_summaries.txt'\n",
    "\n",
    "open(pass_pg_path, 'w').close()\n",
    "open(pass_sum_path, 'w').close()\n",
    "\n",
    "open(removed_pg_path, 'w').close()\n",
    "open(removed_sum_path, 'w').close()\n",
    "\n",
    "with open(pass_pg_path, \"w\") as file:\n",
    "    file.truncate(0)\n",
    "\n",
    "with open(pass_sum_path, \"w\") as file:\n",
    "    file.truncate(0)\n",
    "\n",
    "with open(removed_pg_path, \"w\") as file:\n",
    "    file.truncate(0)\n",
    "\n",
    "with open(removed_sum_path, \"w\") as file:\n",
    "    file.truncate(0)\n",
    "    \n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    preprocessed_Paragraph, removed_Paragraph = preprocess(row['paragraph'])\n",
    "    preprocessed_Summary,removed_Summary  = preprocess(row['summary'])\n",
    "    \n",
    "    for word in preprocessed_Paragraph:\n",
    "        Save_Passed(word, pass_pg_path)\n",
    "\n",
    "    for word in removed_Paragraph:\n",
    "        Save_Removed(word, removed_pg_path)\n",
    "        \n",
    "    for word in preprocessed_Summary:\n",
    "        Save_Passed(word, pass_sum_path)\n",
    "\n",
    "    for word in removed_Summary:\n",
    "        Save_Removed(word, removed_sum_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
