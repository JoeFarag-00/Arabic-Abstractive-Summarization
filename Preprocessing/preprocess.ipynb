{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import camel_tools\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import qalsadi.lemmatizer\n",
    "from camel_tools.ner import NERecognizer\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.dediac import dediac_ar, dediac_bw\n",
    "\n",
    "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "path = \"C://Users//youss//Desktop//My Crap//Geek Mode//Projects//Coding Projects//My Personal Projects//AI Projects//AIC Arabic TSUM//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     example_id                                          paragraph  \\\n",
      "0             0  وتحت عنوان من الكارثة إلى التحدى يبدأ الكاتب ع...   \n",
      "1             1  ولم يعترف دبلوماسيو هاتين الدولتين بالعريضة ال...   \n",
      "2             2  قامت ولاية حلب بعد اعلان الجنرال الفرنسي هنري ...   \n",
      "3             3  دولة مصر العربيه هي ليست اي دوله وليست اي شعب ...   \n",
      "4             4  السوريون يصرون على استقلال بلادهم : و مثلما رف...   \n",
      "..          ...                                                ...   \n",
      "149         149  حزب الوفد سيحتفل بمئوية ثورة 1919 يوم 9 مارس ا...   \n",
      "150         150  حيث أعلن مجلس قيادة الثورة في 18 يونيه 1953 قي...   \n",
      "151         151  وبرغم أن عبد الرحمن فهمي كان يضم في ذلك الجهاز...   \n",
      "152         152  ولم تقتصر مقومات بورسعيد كمدينة عالمية منذ نشأ...   \n",
      "153         153  أول رئيس للجزائر بعد الاستقلال الرئيس أحمد بن ...   \n",
      "\n",
      "                                               summary  \n",
      "0    يبدأ الكاتب عرض الكتاب الرابع تحت عنوان من الك...  \n",
      "1    دبلوماسيو الدولتين لم يعترفوا بالعريضة التي قا...  \n",
      "2    أعلن غورو الانتداب الفرنسي على سوريا لكي يعاقب...  \n",
      "3    مصر هي أم البلاد، وقائدة العرب؛ فهي أرض بلاد ا...  \n",
      "4    الشعب السوري يصر على استقلال بلدهم من السيطرة ...  \n",
      "..                                                 ...  \n",
      "149  احتفال مئوية ثورة 1919 كان من منطلق وطني ليس ح...  \n",
      "150  مجلس قيادة الثورة أعلن عن قيام الجمهورية المصر...  \n",
      "151  ضم عبد الرحمن فهمي في الجهاز السري عدد كبير من...  \n",
      "152  امتدت بورسعيد لكي تشمل الطابع الثقافي للمدينة،...  \n",
      "153  كان أحمد بن بيلا أول رئيس للجزائر بعد الاستقلا...  \n",
      "\n",
      "[154 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(path + 'Dataset//AIC Val//ds.jsonl', lines=True)\n",
    "\n",
    "with open(Path(path + \"Preprocessing//Stopwords//Stopwords_List_HF.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    arabic_stopwords = set(f.read().splitlines())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(text):\n",
    "    return simple_word_tokenize(text)\n",
    "\n",
    "def Lemmatize(text):\n",
    "   return lemmer.lemmatize_text(' '.join(text))\n",
    "\n",
    "def DeDiacritics(text):\n",
    "    return dediac_ar(text)\n",
    "\n",
    "def Remove_SW(tokens, stopwords):\n",
    "    removed_tokens = []\n",
    "    passed_tokens = []\n",
    "    for token in tokens:\n",
    "        if token in stopwords:\n",
    "            removed_tokens.append(token)\n",
    "        else:\n",
    "            passed_tokens.append(token)\n",
    " \n",
    "    return passed_tokens, removed_tokens\n",
    "\n",
    "def Remove_SW_Camel(tokens, stopwords):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def Preprocess_Qalsadi(text):\n",
    "    #TBD REQUIRES IMPLEMENTATION\n",
    "    pass\n",
    "\n",
    "def Preprocess(text):\n",
    "    tokens = DeDiacritics(text)\n",
    "    tokens = Tokenize(tokens)\n",
    "    tokens, rm_tokens = Remove_SW(tokens, arabic_stopwords)\n",
    "    tokens = Lemmatize(tokens)\n",
    "    return tokens,rm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Passed(word, output):\n",
    "    with open(output, 'a', encoding='utf-8') as file:\n",
    "        file.write(word + '\\n')\n",
    "        \n",
    "def Save_Removed(word, output):\n",
    "    with open(output, 'a', encoding='utf-8') as file:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "pass_pg_path = path + 'Debug/Token Tracking/Passed_Tokens/passed_paragraphs.txt'\n",
    "pass_sum_path = path + 'Debug/Token Tracking/Passed_Tokens/passed_summaries.txt'\n",
    "\n",
    "removed_pg_path = path + 'Debug/Token Tracking/Removed_Tokens/removed_paragraphs.txt'\n",
    "removed_sum_path = path + 'Debug/Token Tracking/Removed_Tokens/removed_summaries.txt'\n",
    "\n",
    "processed_records =  path + \"Debug/Processed Dataset/Processed_Dataset.csv\"\n",
    "\n",
    "open(pass_pg_path, 'w').close()\n",
    "open(pass_sum_path, 'w').close()\n",
    "\n",
    "open(removed_pg_path, 'w').close()\n",
    "open(removed_sum_path, 'w').close()\n",
    "\n",
    "with open(pass_pg_path, \"w\") as file:\n",
    "    file.truncate(0)\n",
    "\n",
    "with open(pass_sum_path, \"w\") as file:\n",
    "    file.truncate(0)\n",
    "\n",
    "with open(removed_pg_path, \"w\") as file:\n",
    "    file.truncate(0)\n",
    "\n",
    "with open(removed_sum_path, \"w\") as file:\n",
    "    file.truncate(0)\n",
    "\n",
    "processed_data = []\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    preprocessed_Paragraph, removed_Paragraph = Preprocess(row['paragraph'])\n",
    "    preprocessed_Summary, removed_Summary = Preprocess(row['summary'])\n",
    "    \n",
    "    processed_data.append({'Paragraph': ' '.join(preprocessed_Paragraph), 'Summary': ' '.join(preprocessed_Summary)})\n",
    "    \n",
    "    for word in preprocessed_Paragraph:\n",
    "        Save_Passed(word, pass_pg_path)\n",
    "\n",
    "    for word in removed_Paragraph:\n",
    "        Save_Removed(word, removed_pg_path)\n",
    "        \n",
    "    for word in preprocessed_Summary:\n",
    "        Save_Passed(word, pass_sum_path)\n",
    "\n",
    "    for word in removed_Summary:\n",
    "        Save_Removed(word, removed_sum_path)\n",
    "\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "processed_df.to_csv(processed_records, index=False)\n",
    "print(\"Processed Dataset Records: \", final_records_ct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
