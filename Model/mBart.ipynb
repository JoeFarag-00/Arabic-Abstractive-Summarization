{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\Cuda\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Anaconda\\envs\\Cuda\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Anaconda\\envs\\Cuda\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Anaconda\\envs\\Cuda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>وتحت عنوان من الكارثة إلى التحدى يبدأ الكاتب ع...</td>\n",
       "      <td>يبدأ الكاتب عرض الكتاب الرابع تحت عنوان من الك...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ولم يعترف دبلوماسيو هاتين الدولتين بالعريضة ال...</td>\n",
       "      <td>دبلوماسيو الدولتين لم يعترفوا بالعريضة التي قا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>قامت ولاية حلب بعد اعلان الجنرال الفرنسي هنري ...</td>\n",
       "      <td>أعلن غورو الانتداب الفرنسي على سوريا لكي يعاقب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>دولة مصر العربيه هي ليست اي دوله وليست اي شعب ...</td>\n",
       "      <td>مصر هي أم البلاد، وقائدة العرب؛ فهي أرض بلاد ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>السوريون يصرون على استقلال بلادهم : و مثلما رف...</td>\n",
       "      <td>الشعب السوري يصر على استقلال بلدهم من السيطرة ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id                                          paragraph  \\\n",
       "0           0  وتحت عنوان من الكارثة إلى التحدى يبدأ الكاتب ع...   \n",
       "1           1  ولم يعترف دبلوماسيو هاتين الدولتين بالعريضة ال...   \n",
       "2           2  قامت ولاية حلب بعد اعلان الجنرال الفرنسي هنري ...   \n",
       "3           3  دولة مصر العربيه هي ليست اي دوله وليست اي شعب ...   \n",
       "4           4  السوريون يصرون على استقلال بلادهم : و مثلما رف...   \n",
       "\n",
       "                                             summary  \n",
       "0  يبدأ الكاتب عرض الكتاب الرابع تحت عنوان من الك...  \n",
       "1  دبلوماسيو الدولتين لم يعترفوا بالعريضة التي قا...  \n",
       "2  أعلن غورو الانتداب الفرنسي على سوريا لكي يعاقب...  \n",
       "3  مصر هي أم البلاد، وقائدة العرب؛ فهي أرض بلاد ا...  \n",
       "4  الشعب السوري يصر على استقلال بلدهم من السيطرة ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_json('../Datasets/AIC Val/ds.jsonl', lines=True)\n",
    "DF.head()\n",
    "# DF = pd.read_csv('../Datasets\\WikiHow\\wikiHow.csv', nrows=2000)\n",
    "# DF.columns = ['summary', 'paragraph']\n",
    "# DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paragraphs = DF['paragraph'].tolist()\n",
    "Summaries = DF['summary'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Paragraphs, Test_Paragraphs, Train_Summaries, Test_Summaries = train_test_split(Paragraphs, Summaries, test_size=0.2, random_state=42)\n",
    "Train_Paragraphs, Validation_Paragraphs, Train_Summaries, Validation_Summaries = train_test_split(Train_Paragraphs, Train_Summaries, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBart50Tokenizer'. \n",
      "The class this function is called from is 'MBartTokenizer'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "Model_Name = \"facebook/mbart-large-50\"\n",
    "Tokenizer = MBartTokenizer.from_pretrained(Model_Name)\n",
    "Model = MBartForConditionalGeneration.from_pretrained(Model_Name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Paragraph_Max_Length = 512\n",
    "Train_Paragraph_Encodings = Tokenizer(Train_Paragraphs, truncation=True, max_length=Paragraph_Max_Length, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "Validation_Paragraph_Encodings = Tokenizer(Validation_Paragraphs, truncation=True, max_length=Paragraph_Max_Length, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "Test_Paragraph_Encodings = Tokenizer(Test_Paragraphs, truncation=True, max_length=Paragraph_Max_Length, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "Summary_Max_Length = 64\n",
    "Train_Summary_Encodings = Tokenizer(Train_Summaries, truncation=True, max_length=Summary_Max_Length, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "Validation_Summary_Encodings = Tokenizer(Validation_Summaries, truncation=True, max_length=Summary_Max_Length, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "Test_Summary_Encodings = Tokenizer(Test_Summaries, truncation=True, max_length=Summary_Max_Length, padding=\"max_length\", return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask):\n",
    "        self.input_ids = input_ids.to(device)\n",
    "        self.attention_mask = attention_mask.to(device)\n",
    "        self.decoder_input_ids = decoder_input_ids.to(device)\n",
    "        self.decoder_attention_mask = decoder_attention_mask.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"decoder_input_ids\": self.decoder_input_ids[idx],\n",
    "            \"decoder_attention_mask\": self.decoder_attention_mask[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Dataset = CustomDataset(\n",
    "    Train_Paragraph_Encodings['input_ids'],\n",
    "    Train_Paragraph_Encodings['attention_mask'],\n",
    "    Train_Summary_Encodings['input_ids'],\n",
    "    Train_Summary_Encodings['attention_mask']\n",
    ")\n",
    "\n",
    "Validation_Dataset = CustomDataset(\n",
    "    Validation_Paragraph_Encodings['input_ids'],\n",
    "    Validation_Paragraph_Encodings['attention_mask'],\n",
    "    Validation_Summary_Encodings['input_ids'],\n",
    "    Validation_Summary_Encodings['attention_mask']\n",
    ")\n",
    "\n",
    "Test_Dataset = CustomDataset(\n",
    "    Test_Paragraph_Encodings['input_ids'],\n",
    "    Test_Paragraph_Encodings['attention_mask'],\n",
    "    Test_Summary_Encodings['input_ids'],\n",
    "    Test_Summary_Encodings['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Dataloader = DataLoader(Train_Dataset, batch_size=1)\n",
    "Validation_Dataloader = DataLoader(Validation_Dataset, batch_size=1)\n",
    "Test_Dataloader = DataLoader(Test_Dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/98 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   6%|▌         | 6/98 [00:41<10:36,  6.92s/batch, Loss_Average=0.496, Rouge1_Average=0, Rouge2_Average=0, RougeL_Average=0, loss=7.07]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m Loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     44\u001b[0m Optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m---> 46\u001b[0m Reference_sentences \u001b[39m=\u001b[39m Tokenizer\u001b[39m.\u001b[39;49mbatch_decode(Decoder_Input_IDs, skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     47\u001b[0m Generated_sentences \u001b[39m=\u001b[39m Tokenizer\u001b[39m.\u001b[39mbatch_decode(Outputs\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m Scores \u001b[39m=\u001b[39m Scorer\u001b[39m.\u001b[39mscore(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(Reference_sentences), \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(Generated_sentences))\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Cuda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3446\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[0;32m   3423\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   3424\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3427\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3428\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   3429\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3430\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3431\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3444\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3446\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   3447\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[0;32m   3448\u001b[0m             seq,\n\u001b[0;32m   3449\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3450\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3451\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3452\u001b[0m         )\n\u001b[0;32m   3453\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[0;32m   3454\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Cuda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3447\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[0;32m   3423\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   3424\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3427\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3428\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   3429\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3430\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   3431\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3444\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   3445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3446\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m-> 3447\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[0;32m   3448\u001b[0m             seq,\n\u001b[0;32m   3449\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3450\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3451\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3452\u001b[0m         )\n\u001b[0;32m   3453\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[0;32m   3454\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Cuda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3484\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3463\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3464\u001b[0m \u001b[39mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m   3465\u001b[0m \u001b[39mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3481\u001b[0m \u001b[39m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m   3482\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3483\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m-> 3484\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m   3486\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[0;32m   3487\u001b[0m     token_ids\u001b[39m=\u001b[39mtoken_ids,\n\u001b[0;32m   3488\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   3489\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   3490\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   3491\u001b[0m )\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\Cuda\\lib\\site-packages\\transformers\\utils\\generic.py:193\u001b[0m, in \u001b[0;36mto_py_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    192\u001b[0m \u001b[39melif\u001b[39;00m is_torch_tensor(obj):\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m    194\u001b[0m \u001b[39melif\u001b[39;00m is_jax_tensor(obj):\n\u001b[0;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(obj)\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Optimizer = torch.optim.AdamW(Model.parameters(), lr=1e-5)\n",
    "Criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "Epochs_Number = 5\n",
    "\n",
    "for Epoch in range(Epochs_Number):\n",
    "    Model.train()\n",
    "    Total_Loss = 0\n",
    "\n",
    "    Total_Rouge1 = 0.0\n",
    "    Total_Rouge2 = 0.0\n",
    "    Total_RougeL = 0.0\n",
    "    Total_Batches = 0\n",
    "    with tqdm(Train_Dataloader, desc=f\"Epoch {Epoch + 1}/{Epochs_Number}\",  unit=\"batch\") as t:\n",
    "        for Batch in t:\n",
    "            Input_IDs = Batch[\"input_ids\"].to(device)\n",
    "            Attention_Mask = Batch[\"attention_mask\"].to(device)\n",
    "            Decoder_Input_IDs = Batch[\"decoder_input_ids\"].to(device)\n",
    "            Decoder_Attention_Mask = Batch[\"decoder_attention_mask\"].to(device)\n",
    "\n",
    "            Outputs = Model(\n",
    "                input_ids=Input_IDs,\n",
    "                attention_mask=Attention_Mask,\n",
    "                decoder_input_ids=Decoder_Input_IDs,\n",
    "                decoder_attention_mask=Decoder_Attention_Mask,\n",
    "                labels=Decoder_Input_IDs\n",
    "            )\n",
    "\n",
    "            # IDs = Tokenizer.batch_decode(Input_IDs, skip_special_tokens=True)\n",
    "            # decoded_output = Tokenizer.batch_decode(Outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n",
    "            # print(IDs)\n",
    "            # print(decoded_output)\n",
    "\n",
    "            Loss = Outputs.loss\n",
    "            Total_Loss += Loss.item()\n",
    "\n",
    "            Optimizer.zero_grad()\n",
    "            Loss.backward()\n",
    "            Optimizer.step()\n",
    "\n",
    "            Reference_sentences = Tokenizer.batch_decode(Decoder_Input_IDs, skip_special_tokens=True)\n",
    "            Generated_sentences = Tokenizer.batch_decode(Outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n",
    "\n",
    "            Scores = Scorer.score(''.join(Reference_sentences), ''.join(Generated_sentences))\n",
    "            Total_Rouge1 += Scores['rouge1'].fmeasure\n",
    "            Total_Rouge2 += Scores['rouge2'].fmeasure\n",
    "            Total_RougeL += Scores['rougeL'].fmeasure\n",
    "            Total_Batches += 1\n",
    "\n",
    "            t.set_postfix(\n",
    "                Loss_Average=Total_Loss / len(Train_Dataloader),\n",
    "                loss=Loss.item(),\n",
    "                Rouge1_Average=Total_Rouge1 / Total_Batches,\n",
    "                Rouge2_Average=Total_Rouge2 / Total_Batches,\n",
    "                RougeL_Average=Total_RougeL / Total_Batches\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Save_Path = 'mBart.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Model.state_dict(), Model_Save_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.load_state_dict(torch.load(Model_Save_Path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence_Transformer_Model = SentenceTransformer('distiluse-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 31/31 [01:52<00:00,  3.64s/batch, Loss_Average=7.54, Rouge1_Average=0.226, Rouge2_Average=0.0968, RougeL_Average=0.226, Semantic_Similarity_Average=0.679, Variance_Average=0.0118, loss=23.1] \n"
     ]
    }
   ],
   "source": [
    "Total_Loss = 0.0\n",
    "Total_Rouge1 = 0.0\n",
    "Total_Rouge2 = 0.0\n",
    "Total_RougeL = 0.0\n",
    "Total_Similarity = 0.0\n",
    "Total_Variance = 0.0\n",
    "Similarity_Average = 0\n",
    "Variance_Average = 0\n",
    "Total_Batches = 0\n",
    "with tqdm(Test_Dataloader, desc=\"Testing: \",  unit=\"batch\") as t:\n",
    "    for Batch in t:\n",
    "        Input_IDs = Batch[\"input_ids\"].to(device)\n",
    "        Attention_Mask = Batch[\"attention_mask\"].to(device)\n",
    "        Decoder_Input_IDs = Batch[\"decoder_input_ids\"].to(device)\n",
    "        Decoder_Attention_Mask = Batch[\"decoder_attention_mask\"].to(device)\n",
    "\n",
    "\n",
    "        Outputs = Model(\n",
    "            input_ids=Input_IDs,\n",
    "            attention_mask=Attention_Mask,\n",
    "            labels=Decoder_Input_IDs\n",
    "        )\n",
    "\n",
    "        Input_Text = Tokenizer.batch_decode(Input_IDs, skip_special_tokens=True)\n",
    "        Generated_sentences = Tokenizer.batch_decode(Outputs.logits.argmax(dim=-1), skip_special_tokens=True)\n",
    "        # print(Input_Text)\n",
    "        # print(Generated_sentences)\n",
    "\n",
    "        Loss = Outputs.loss\n",
    "        Total_Loss += Loss.item()\n",
    "\n",
    "        Loss.backward()\n",
    "\n",
    "        Reference_sentences = Tokenizer.batch_decode(Decoder_Input_IDs, skip_special_tokens=True)\n",
    "\n",
    "        Scores = Scorer.score(''.join(Reference_sentences), ''.join(Generated_sentences))\n",
    "        Total_Rouge1 += Scores['rouge1'].fmeasure\n",
    "        Total_Rouge2 += Scores['rouge2'].fmeasure\n",
    "        Total_RougeL += Scores['rougeL'].fmeasure\n",
    "        Total_Batches += 1\n",
    "\n",
    "\n",
    "        Embeddings = Sentence_Transformer_Model.encode([str(Input_Text), str(Generated_sentences)], convert_to_tensor=True)\n",
    "        Cos_Sim = util.pytorch_cos_sim(Embeddings[0], Embeddings[1])\n",
    "        Cos_Sim_Value = Cos_Sim.item()\n",
    "        Total_Similarity += Cos_Sim_Value\n",
    "        Similarity_Average = Total_Similarity / Total_Batches\n",
    "\n",
    "        t.set_postfix(\n",
    "            Loss_Average=Total_Loss / len(Train_Dataloader),\n",
    "            loss=Loss.item(),\n",
    "            Rouge1_Average=Total_Rouge1 / Total_Batches,\n",
    "            Rouge2_Average=Total_Rouge2 / Total_Batches,\n",
    "            RougeL_Average=Total_RougeL / Total_Batches,\n",
    "            Semantic_Similarity_Average = Similarity_Average,\n",
    "            Variance_Average = Variance_Average\n",
    "        )\n",
    "\n",
    "        Total_Variance += (Cos_Sim_Value - Similarity_Average) ** 2\n",
    "        Variance_Average = Total_Variance / Total_Batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
